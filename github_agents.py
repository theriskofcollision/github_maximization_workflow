import os
import yaml
import logging
import requests
from typing import List, Dict, Optional, Callable
from typing import List, Dict, Optional, Callable
from collections import deque
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("GitHubAgents")

# --- Event Bus ---
class Event:
    def __init__(self, type: str, payload: Dict):
        self.type = type
        self.payload = payload

class EventBus:
    def __init__(self):
        self.subscribers: Dict[str, List[Callable]] = {}
        self.event_queue = deque()

    def subscribe(self, event_type: str, handler: Callable):
        if event_type not in self.subscribers:
            self.subscribers[event_type] = []
        self.subscribers[event_type].append(handler)

    def publish(self, event: Event):
        logger.info(f"[EventBus] Event Published: {event.type}")
        self.event_queue.append(event)

    def process_events(self):
        while self.event_queue:
            event = self.event_queue.popleft()
            if event.type in self.subscribers:
                for handler in self.subscribers[event.type]:
                    handler(event)

# --- Agents ---
class Agent:
    def __init__(self, name: str, role: str, goal: str, system_prompt: str, event_bus: EventBus = None):
        self.name = name
        self.role = role
        self.goal = goal
        self.system_prompt = system_prompt
        self.event_bus = event_bus

    def run(self, task: str, context: Dict = None) -> str:
        """
        Executes the agent's task using Google Gemini.
        """
        logger.info(f"[{self.name}] Starting task: {task}")
        
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            logger.warning("GEMINI_API_KEY not found. Running in simulation mode.")
            return f"[{self.name}] Completed task: {task}. (Simulation - No API Key)"

        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-2.0-flash')
            
            prompt = f"""
            System: {self.system_prompt}
            Role: {self.role}
            Goal: {self.goal}
            Context: {context if context else {}}
            Task: {task}
            """
            
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.error(f"Error calling Gemini API: {e}")
            return f"Error: {e}"

class RepoGardener(Agent):
    def audit_repo(self, repo_path: str):
        logger.info(f"Auditing repo at {repo_path}...")
        # Logic to check for README, LICENSE, etc.
        if not os.path.exists(os.path.join(repo_path, "README.md")):
            logger.warning(f"Missing README in {repo_path}")
            if self.event_bus:
                self.event_bus.publish(Event("MISSING_FILE", {"repo": repo_path, "file": "README.md"}))
            return "Missing README"
        return "Repo looks healthy"

    def audit_repo_api(self, repo_name: str, check_file_exists_api_func: Callable, check_workflow_status_func: Callable, create_issue_func: Callable):
        logger.info(f"Auditing {repo_name}...")
        
        # 1. Check for README
        if not check_file_exists_api_func(repo_name, "README.md"):
             logger.warning(f"Missing README in {repo_name}")
             if self.event_bus:
                 self.event_bus.publish(Event("MISSING_FILE", {"repo": repo_name, "file": "README.md", "is_remote": True}))
        
        # 2. Check Workflow Status
        workflow_errors = check_workflow_status_func(repo_name)
        if workflow_errors:
            for error in workflow_errors:
                logger.error(f"Workflow Failure in {repo_name}: {error}")
                # Create GitHub Issue
                create_issue_func(repo_name, "ðŸš¨ Workflow Failure Detected", f"The RepoGardener agent detected a workflow failure:\n\n{error}\n\nPlease investigate immediately.")
        
        if not workflow_errors and check_file_exists_api_func(repo_name, "README.md"):
            logger.info(f"{repo_name} is healthy.")

class CodeArchitect(Agent):
    def scaffold_project(self, project_name: str, stack: str):
        logger.info(f"Scaffolding {project_name} with {stack}...")
        # Logic to create directories and files
        os.makedirs(project_name, exist_ok=True)
        with open(f"{project_name}/README.md", "w") as f:
            f.write(f"# {project_name}\n\nGenerated by CodeArchitect.")
        return f"Project {project_name} created."

    def handle_missing_file(self, event: Event):
        repo = event.payload['repo']
        file = event.payload['file']
        logger.info(f"[{self.name}] Handling MISSING_FILE: {file} in {repo}")
        
        # Generate content using LLM
        prompt = f"Generate a professional {file} for the repository {repo}. IMPORTANT: Output ONLY the raw file content. Do not include any conversational text, 'Here is the file', or markdown code fences (```). Start directly with the file content."
        content = self.run(prompt)
        
        # Clean up the response (remove markdown fences if present)
        if content.startswith("```"):
            content = content.split("\n", 1)[1]
            if content.endswith("```"):
                content = content.rsplit("\n", 1)[0]
        
        # Remove any leading conversational text if it doesn't look like a file start
        # Simple heuristic: If it doesn't start with # or < or import or package, and has a double newline, split it.
        # But the prompt engineering above should handle most cases.
        
        logger.info(f"[{self.name}] Generated content for {file}:\n{content[:100]}...")
        
        # Commit to GitHub
        try:
            commit_url = commit_file_to_github(repo, file, content, f"Create {file} via CodeArchitect Agent")
            logger.info(f"[{self.name}] Successfully committed {file} to {repo}. URL: {commit_url}")
        except Exception as e:
            logger.error(f"[{self.name}] Failed to commit {file}: {e}")

    # commit_file_to_github moved to global scope

class TrendSurfer(Agent):
    def find_trends(self, topics: List[str]) -> List[str]:
        logger.info(f"Scanning trends for: {topics}")
        headers = get_github_headers()
        trends = []
        
        # Search for recent high-star repos in these topics
        # Broader query to ensure results
        query = f"topic:{topics[0]} sort:stars order:desc"
        url = f"https://api.github.com/search/repositories?q={query}&per_page=3"
        
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                items = response.json().get('items', [])
                for item in items:
                    trends.append(f"{item['full_name']} - â­ {item['stargazers_count']} ({item['html_url']})")
        except Exception as e:
            logger.error(f"Failed to find trends: {e}")
            
        return trends

class ProfilePolisher(Agent):
    def update_profile(self, stats: Dict):
        logger.info(f"Updating profile with stats: {stats}")
        # Logic to update README.md
        # For now, we'll just log what we WOULD do, or append to a local file if it exists
        
        content = f"\n\n## ðŸ¤– Agentic Update\n- **Total Repos**: {stats.get('total_repos', 0)}\n- **Last Scan**: {stats.get('last_scan', 'N/A')}\n"
        
        # Update README via GitHub API
        # We need the current repo name. In 'all' scope, this agent runs once at the end.
        # For simplicity, we'll assume we are updating the CURRENT repo's README.
        # In a real scenario, we might pass the repo name.
        # Let's try to infer it or use a config.
        
        # For this demo, we'll try to find the repo name from the environment or config
        # If running in GitHub Actions, GITHUB_REPOSITORY is available.
        repo_full_name = os.getenv("GITHUB_REPOSITORY")
        
        if not repo_full_name:
             return "Skipping Profile Update: GITHUB_REPOSITORY env var not found (local run?)"

        # Fetch current README content to append, or just append to the end
        # Since API replaces content, we need to read -> append -> write
        
        try:
            # 1. Get current content
            headers = get_github_headers()
            url = f"https://api.github.com/repos/{repo_full_name}/contents/README.md"
            response = requests.get(url, headers=headers)
            
            current_content = ""
            sha = None
            if response.status_code == 200:
                import base64
                content_data = response.json()
                current_content = base64.b64decode(content_data['content']).decode('utf-8')
                sha = content_data['sha']
            
            # 2. Append stats
            new_section = f"\n\n## ðŸ¤– Agentic Update\n- **Total Repos**: {stats.get('total_repos', 0)}\n- **Last Scan**: {stats.get('last_scan', 'N/A')}\n"
            
            # Avoid duplicate sections if running multiple times
            if "## ðŸ¤– Agentic Update" in current_content:
                # Replace the existing section (simple split)
                current_content = current_content.split("## ðŸ¤– Agentic Update")[0].strip()
            
            final_content = current_content + new_section
            
            # 3. Commit
            commit_url = commit_file_to_github(repo_full_name, "README.md", final_content, "Update Profile Stats via ProfilePolisher")
            return f"Profile README.md updated: {commit_url}"
            
        except Exception as e:
            return f"Failed to update profile: {e}"

def get_github_headers():
    token = os.getenv("GH_PAT") or os.getenv("GITHUB_TOKEN")
    if not token:
        raise ValueError("No GitHub token found. Please set GH_PAT or GITHUB_TOKEN.")
    return {"Authorization": f"Bearer {token}", "Accept": "application/vnd.github.v3+json"}

def commit_file_to_github(repo_full_name: str, file_path: str, content: str, commit_message: str) -> str:
    """
    Commits a file to a GitHub repository using the API.
    """
    headers = get_github_headers()
    url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
    
    # Check if file exists to get SHA (for updates)
    sha = None
    try:
        get_response = requests.get(url, headers=headers)
        if get_response.status_code == 200:
            sha = get_response.json()['sha']
    except Exception:
        pass

    import base64
    content_b64 = base64.b64encode(content.encode('utf-8')).decode('utf-8')
    
    data = {
        "message": commit_message,
        "content": content_b64
    }
    if sha:
        data["sha"] = sha
        
    response = requests.put(url, headers=headers, json=data)
    response.raise_for_status()
    return response.json()['content']['html_url']

def load_config(config_path: str) -> Dict:
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def main():
    config = load_config("workflow_config.yml")
    logger.info(f"Starting {config['project_name']}...")

    # Initialize Event Bus
    event_bus = EventBus()

    # Initialize Agents
    gardener = RepoGardener("RepoGardener", "Maintainer", "Keep repos healthy", "...", event_bus)
    architect = CodeArchitect("CodeArchitect", "Builder", "Scaffold projects", "...", event_bus)
    surfer = TrendSurfer("TrendSurfer", "Scout", "Find trends", "...", event_bus)
    polisher = ProfilePolisher("ProfilePolisher", "Brand Manager", "Polish profile", "...", event_bus)

    # Register Event Handlers
    event_bus.subscribe("MISSING_FILE", architect.handle_missing_file)

    # GitHub API Helper
    # get_github_headers is now global

    def fetch_repos(scope: str = "all") -> List[Dict]:
        headers = get_github_headers()
        repos = []
        
        if scope == "current":
            pass 
        
        # Fetch all repos for the authenticated user
        url = "https://api.github.com/user/repos?per_page=100&type=all"
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            repos = response.json()
        except Exception as e:
            logger.error(f"Failed to fetch repos: {e}")
            
        return repos

    def create_github_issue(repo_full_name: str, title: str, body: str):
        headers = get_github_headers()
        url = f"https://api.github.com/repos/{repo_full_name}/issues"
        data = {"title": title, "body": body}
        try:
            # Check if issue already exists to avoid duplicates (simple check)
            # In a real app, we'd search for open issues with the same title
            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 201:
                logger.info(f"Created issue in {repo_full_name}: {title}")
            else:
                logger.error(f"Failed to create issue: {response.text}")
        except Exception as e:
            logger.error(f"Error creating issue: {e}")

    def check_file_exists_api(repo_full_name: str, file_path: str) -> bool:
        headers = get_github_headers()
        url = f"https://api.github.com/repos/{repo_full_name}/contents/{file_path}"
        try:
            response = requests.get(url, headers=headers)
            return response.status_code == 200
        except Exception:
            return False

    def check_workflow_status(repo_full_name: str) -> List[str]:
        headers = get_github_headers()
        # Fetch recent runs to cover multiple workflows
        url = f"https://api.github.com/repos/{repo_full_name}/actions/runs?per_page=20"
        errors = []
        seen_workflows = set()
        
        try:
            response = requests.get(url, headers=headers)
            if response.status_code == 200:
                data = response.json()
                for run in data.get('workflow_runs', []):
                    workflow_id = run['workflow_id']
                    
                    # Only check the LATEST run of each workflow
                    if workflow_id not in seen_workflows:
                        seen_workflows.add(workflow_id)
                        
                        if run['conclusion'] == 'failure':
                            errors.append(f"Workflow '{run['name']}' failed (Run #{run['run_number']}). URL: {run['html_url']}")
        except Exception as e:
            logger.error(f"Failed to check workflows for {repo_full_name}: {e}")
        return errors

    # Workflow Execution
    if config['agents']['repo_gardener']['enabled']:
        scope = config['agents']['repo_gardener'].get('audit_scope', 'current')
        
        if scope == 'all':
            logger.info("Fetching all repositories...")
            repos = fetch_repos()
            for repo in repos:
                repo_name = repo['full_name']
                gardener.audit_repo_api(repo_name, check_file_exists_api, check_workflow_status, create_github_issue)
        else:
            # Fallback to local check for current repo
            gardener.audit_repo(".")

    if config['agents']['trend_surfer']['enabled']:
        trends = surfer.find_trends(config['agents']['trend_surfer']['topics'])
        logger.info(f"Found trends:\n" + "\n".join(trends))

    if config['agents']['profile_polisher']['enabled']:
        # Calculate simple stats
        import datetime
        stats = {
            "total_repos": len(repos) if 'repos' in locals() else 0,
            "last_scan": datetime.datetime.now().isoformat()
        }
        result = polisher.update_profile(stats)
        logger.info(result)

    # Process Event Queue
    logger.info("Processing Event Queue...")
    event_bus.process_events()

    logger.info("Workflow completed.")

if __name__ == "__main__":
    main()
